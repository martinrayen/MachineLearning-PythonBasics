{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This is the code to run the process that does the vendor ranking.\n",
    "    By : Joseph MTV\n",
    "    \n",
    "    Input  : The file containing the Material/Vendor combo, that needs to be ranked.\n",
    "    Output : The file containing the ranked vendors, exported as .csv file to the configured \"Output\" folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions:\n",
    "    => Feed template has an empty row, preceeding the column header. (Changes to this setup, will require changes \n",
    "       in AppConfig.)\n",
    "    => Input feed file is an .xlsx file.  The active sheet to be considered is to be configured in AppConfig.\n",
    "    => Footer rows will be dropped.\n",
    "    => Multiple material/vendor combo appearing within the time-period is rolledup by averaging the numbers.\n",
    "    => Delivery performance scores are computed for Delivery date and Delivered Qty only.\n",
    "    => Apply the following weighting rules:\n",
    "        1. Deviation in dates  * 0.4\n",
    "        2. Deviation in qty    * 0.4\n",
    "        3. VendorMaterialCount * 0.2\n",
    "    => Ranking is done on \n",
    "        (Deviation in dates  * 0.4) + (Deviation in qty    * 0.4) + (VendorMaterialCount * 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.00]: Import the required modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required modules.\n",
    "import os\n",
    "import re\n",
    "import wx             # Install it with pip.\n",
    "import sys\n",
    "import json\n",
    "import shutil         # Install it with pip.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import path\n",
    "from datetime import datetime\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Ignore warnings.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.10]: Instantiate the ML pipeline class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.11]: Prompt for file upload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prompt the user to upload the file.\n",
    "def get_path(wildcard):\n",
    "    app = wx.App(None)\n",
    "    style = wx.FD_OPEN | wx.FD_FILE_MUST_EXIST\n",
    "    dialog = wx.FileDialog(None, 'Choose the file for ranking vendors.', wildcard=wildcard, style=style)\n",
    "    if dialog.ShowModal() == wx.ID_OK:\n",
    "        path = dialog.GetPath()\n",
    "    else:\n",
    "        path = None\n",
    "    dialog.Destroy()\n",
    "    return path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.12] : Instantiate the ML Pipleline class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\DevEnv\\Projects\\VendorSelection\\KPTL_POC\\Source\\\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # Call the funtion to get the input files.\n",
    "    #s_infile = get_path('*.csv')\n",
    "    s_infile = get_path('*.xlsx')\n",
    "    s_infile = s_infile.replace(\"\\\\\",\"\\\\\\\\\")\n",
    "\n",
    "    # Get the current working directory.\n",
    "    s_path = os.getcwd()\n",
    "    s_path = s_path.replace(\"\\\\\",\"\\\\\\\\\")\n",
    "    s_basefile = os.path.basename(s_infile)\n",
    "    s_basefile_csv = (os.path.splitext(s_basefile)[0]) + '.csv'\n",
    "\n",
    "    # Check, if path exists in system path,else add.\n",
    "    if s_path in os.environ:\n",
    "        sys.path.append(s_path)\n",
    "\n",
    "    # Instantiate and Consume the class.\n",
    "    from classMLModelingPipeline import *\n",
    "    # Open and read the App Configuration using json.\n",
    "    with open(s_path + '\\\\AppConfig.txt') as json_file:\n",
    "        # Load the App config details.\n",
    "        data = json.load(json_file)\n",
    "        # For each entry in json, extract App config parameters.\n",
    "        for p in data['AppConfig']:\n",
    "            applConfig = modMLModelingPipeline  (  p['Id'],\n",
    "                                                    p['Name'],\n",
    "                                                    p['Source'],\n",
    "                                                    p['Output'],\n",
    "                                                    p['TrainedModel'],\n",
    "                                                    p['ExecutionLog'],\n",
    "                                                    p['ExecutionLogFileName'],\n",
    "                                                    p['FeedActiveSheet'],\n",
    "                                                    p['FeedSkipRows'],\n",
    "                                                    p['Archive']\n",
    "                                                  )\n",
    "            print(applConfig.Source)\n",
    "    #===== Create Application Directories ============================================================\n",
    "    s_classApplication  = 'classMLModelingPipeline'\n",
    "    s_classMethod       = 'CreateAppDirectories'\n",
    "    s_statusType        = 'Success'\n",
    "    s_statusDescription = 'Successfull write operation.'\n",
    "    applConfig.CreateAppDirectories()\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )\n",
    "except AttributeError:\n",
    "    s_statusDescription = 'No file has been choosen!!!!' \n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)\n",
    "except:\n",
    "    s_statusDescription = 'Unexpected error : ' \n",
    "    s_statusDescription = s_statusDescription + str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.20]: Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Data Loading'    \n",
    "    s_statusType        = 'Success'\n",
    "    feed_active_sheet   = applConfig.FeedActiveSheet\n",
    "    feed_skiprows       = int(applConfig.FeedSkipRows)\n",
    "    feed_skiprows_list  = list(range(int(feed_skiprows)))\n",
    "    if len(feed_skiprows_list) > 0:\n",
    "        # Read the input file.\n",
    "        data_xls = pd.read_excel(s_infile, feed_active_sheet, index_col=None,skiprows=feed_skiprows_list)\n",
    "    else:\n",
    "        data_xls = pd.read_excel(s_infile, feed_active_sheet, index_col=None)\n",
    "    # Drop empty rows.\n",
    "    data_xls.dropna(axis=0,how='any',inplace=True)\n",
    "    # Get the path to source directory from class.\n",
    "    s_in_source_file = applConfig.Source\n",
    "    s_in_source_file = s_in_source_file + s_basefile_csv\n",
    "\n",
    "    # Convert it to .csv utf-8 format.\n",
    "    data_xls.to_csv(s_in_source_file, encoding='utf-8')\n",
    "    \n",
    "    # Load the data from the csv file into pandas dataframe.\n",
    "    dfDataVendor = pd.read_csv(s_in_source_file\n",
    "                               ,parse_dates = ['Doc. Date','Del Date']\n",
    "                               ,infer_datetime_format = True\n",
    "                              )\n",
    "except:\n",
    "    s_statusType        = 'Error'    \n",
    "    s_statusDescription = 'Unexpected error : ' \n",
    "    s_statusDescription = s_statusDescription + str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Drop Unnamed Column'\n",
    "    s_statusType        = 'Success'\n",
    "    # Drop the unwanted column.\n",
    "    dfDataVendor.drop(columns=['Unnamed: 0'],inplace=True)\n",
    "except KeyError:\n",
    "    pass\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = 'Unexpected error : ' \n",
    "    s_statusDescription = s_statusDescription + str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.30]: Data pre-processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.31]: Extract VendorCode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Extract Vendor Code'\n",
    "    s_statusType        = 'Success'    \n",
    "    # Extract VendorCode from \"Supplier/Supplying Plant\" column.\n",
    "    dfDataVendor['VendorCode']  = dfDataVendor['Supplier/Supplying Plant'].str.extract('(\\d+)')\n",
    "    # Alternate way:\n",
    "    #dfDataVendor['VendorCode']  = dfDataVendor['Supplier/Supplying Plant'].str.extract('([0-9][0-9][0-9][0-9][0-9][0-9]+)')\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = s_statusDescription + str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.32]: Inspect datatypes and perform validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetValidExceptionForDates(lstDateColumns, # list of date columns to validate.\n",
    "                              dfData,          # data frame of the input data.\n",
    "                              infile_skiprows  # number of rows to skip in the begining from the input file.\n",
    "                             ):\n",
    "    # Initialize.\n",
    "    validationException = ''\n",
    "    validException_item = ''\n",
    "    # Corrected Row_index in source file.\n",
    "    cRowIndex = infile_skiprows + 2\n",
    "    # Build the Validation Exception message.\n",
    "    validExceptionPresent = 0\n",
    "    #validExcept_header = \"'ValidationException':\"\n",
    "    validExcept_body   = \"'Incorrect_DateFormat' : [\"\n",
    "    validExcept_bodyfooter = \"]\"\n",
    "    # Loop through the list of Date columns.\n",
    "    for itm in lstDateColumns:\n",
    "        # Validate columns.\n",
    "        dfDateCheck = pd.to_datetime(dfData[itm],errors='coerce')\n",
    "        lstDateExceptions = list(dfDateCheck[dfDateCheck.isnull()].index+cRowIndex)\n",
    "        if len(lstDateExceptions) > 0:\n",
    "            validExceptionPresent = 1\n",
    "            validException_item = validException_item + \"'\" + itm + \"':\" + str(lstDateExceptions) + \",\"\n",
    "\n",
    "    if validExceptionPresent:\n",
    "        # Remove extra comma.\n",
    "        validException_item = validException_item[:-1]\n",
    "        # Build the Validation Exception Message.\n",
    "        validationException = validExcept_body + validException_item + validExcept_bodyfooter\n",
    "    return validationException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetValidExceptionForNumeric(lstNumericColumns,  # list of date columns to validate.\n",
    "                                dfData,          # data frame of the input data.\n",
    "                                infile_skiprows  # number of rows to skip in the begining from the input file.\n",
    "                               ):\n",
    "    # Initialize.\n",
    "    validationException = ''\n",
    "    validException_item = ''\n",
    "    # Corrected Row_index in source file.\n",
    "    cRowIndex = infile_skiprows + 2\n",
    "    # Build the Validation Exception message.\n",
    "    validExceptionPresent = 0\n",
    "    #validExcept_header = \"'ValidationException':\"\n",
    "    validExcept_body   = \"'Incorrect_NumericFormat' : [\"\n",
    "    validExcept_bodyfooter = \"]\"\n",
    "    # Loop through the list of Date columns.\n",
    "    for itm in lstNumericColumns:\n",
    "        # Validate columns.\n",
    "        dfNumericCheck = pd.to_numeric(dfData[itm],errors='coerce')\n",
    "        lstNumericExceptions = list(dfNumericCheck[dfNumericCheck.isnull()].index+cRowIndex)\n",
    "        if len(lstNumericExceptions) > 0:\n",
    "            validExceptionPresent = 1\n",
    "            validException_item = validException_item + \"'\" + itm + \"':\" + str(lstNumericExceptions) + \",\"\n",
    "\n",
    "    if validExceptionPresent:\n",
    "        # Remove extra comma.\n",
    "        validException_item = validException_item[:-1]\n",
    "        # Build the Validation Exception Message.\n",
    "        validationException = validExcept_body + validException_item + validExcept_bodyfooter\n",
    "    return validationException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # Consolidate error description to write to log.\n",
    "    #======= Validate Numeric columns ========================================================================================\n",
    "    raise_Except = 0\n",
    "    validationException = ''\n",
    "    strComma = ''\n",
    "    validExcept_header = \"'ValidationException':\"\n",
    "    s_statusType        = 'ValidationException'    \n",
    "    s_classMethod       = 'Parse Datatypes'\n",
    "    # Convert date column's datatype from string to datetime.\n",
    "    lstNumericCols = ['Sum of PO Quantity','Sum of      Net Price','Sum of PO Value']\n",
    "    exceptMessage = GetValidExceptionForNumeric(lstNumericColumns = lstNumericCols, # list of numeric columns to validate.\n",
    "                                                dfData = dfDataVendor,              # data frame of the input data.\n",
    "                                                infile_skiprows = feed_skiprows     # number of rows to skip in the begining from the input file.\n",
    "                                               )\n",
    "    if len(exceptMessage) > 0:\n",
    "        raise_Except = 1\n",
    "        s_statusDescription = exceptMessage\n",
    "\n",
    "    #======= Validate Date columns =====================================================================================\n",
    "    # set the class name and status.\n",
    "    # Convert date column's datatype from string to datetime.\n",
    "    lstDateCols = ['Doc. Date','Del Date']\n",
    "    # Get the validation exceptions.\n",
    "    exceptMessage = GetValidExceptionForDates(lstDateColumns = lstDateCols,    # list of date columns to validate.\n",
    "                                              dfData = dfDataVendor,           # data frame of the input data.\n",
    "                                              infile_skiprows = feed_skiprows  # number of rows to skip in the begining from the input file.\n",
    "                                             )\n",
    "    # If validation exception found.\n",
    "    if len(exceptMessage) > 0:\n",
    "        if raise_Except == 1:\n",
    "            strComma = ','\n",
    "        raise_Except = 1\n",
    "        s_statusDescription = s_statusDescription + strComma + exceptMessage\n",
    "    \n",
    "    if raise_Except:\n",
    "        validationException = validExcept_header + '{' + s_statusDescription + '}'\n",
    "        # Write to log.\n",
    "        applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                      s_classMethod,\n",
    "                                      s_statusType,\n",
    "                                      s_statusDescription\n",
    "                                     ) \n",
    "        \n",
    "        raise Exception('Data validation error/s occured. Please check the application log.')\n",
    "        \n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = validationException + str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.33]: Export data cleansed, so far to .csv format for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the date columns to a csv file and manually inspect for anamolies.\n",
    "dfDataVendor[['Doc. Date','Del Date','VendorCode']].to_csv('Data_Inspect_prediction.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.34]: Get the latest data for multiple vendor occurence for the given snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Resolve Multiple Material-Vendor combo'\n",
    "    s_statusType        = 'Success'    \n",
    "    # Get the latest data for material/combo appearing more than once.\n",
    "    dfDataVendorMaxDate = dfDataVendor.groupby(['Material', \\\n",
    "                                                'UOM', \\\n",
    "                                                'VendorCode']).agg({'Doc. Date':'max', \\\n",
    "                                                                    'Del Date' : 'max', \\\n",
    "                                                                    'Sum of PO Quantity':'mean', \\\n",
    "                                                                    'Sum of      Net Price':'mean', \\\n",
    "                                                                    'Sum of PO Value' : 'mean' \\\n",
    "                                                                    }) \\\n",
    "                          .reindex(['Doc. Date','Del Date', \\\n",
    "                                    'Sum of PO Quantity', \\\n",
    "                                    'Sum of      Net Price', \\\n",
    "                                    'Sum of PO Value' \\\n",
    "                                   ], axis=1) \\\n",
    "                          .reset_index()\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rollup of data.\n",
    "#dfDataVendorMaxDate = dfDataVendor[['Material','UOM','VendorCode','Doc. Date','Del Date','Sum of PO Quantity',\n",
    "#                                    'Sum of      Net Price','Sum of PO Value']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.35]: Get the delivery performance parameters from the pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Get the deviation metrics'\n",
    "    s_statusType        = 'Success'    \n",
    "\n",
    "    # Get the delivery performance parameters from the pickle folder.\n",
    "    dfDeliveryPerformanceParameters = applConfig.ReadDataframeFromPickle()\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the latest data for material/combo appearing more than once.\n",
    "dfDeliveryPerformanceParameters = \\\n",
    "dfDeliveryPerformanceParameters.groupby(['Material', \\\n",
    "                                         'VendorCode']).agg({'Deviation_DeliveryDate':'mean', \\\n",
    "                                                             'Deviation_DeliveredQty' : 'mean', \\\n",
    "                                                             'Deviation_DeliveredValue':'mean', \\\n",
    "                                                             'MaterialCountByVendor':'max' \\\n",
    "                                                            }) \\\n",
    "                      .reindex(['Deviation_DeliveryDate', \\\n",
    "                                'Deviation_DeliveredQty', \\\n",
    "                                'Deviation_DeliveredValue', \\\n",
    "                                'MaterialCountByVendor' \\\n",
    "                               ], axis=1) \\\n",
    "                      .reset_index()\n",
    "dfDeliveryPerformanceParameters['Deviation_DeliveryDate'] = dfDeliveryPerformanceParameters['Deviation_DeliveryDate']. \\\n",
    "                                                            round(2)\n",
    "dfDeliveryPerformanceParameters['Deviation_DeliveredQty'] = dfDeliveryPerformanceParameters['Deviation_DeliveredQty']. \\\n",
    "                                                            round(2)\n",
    "dfDeliveryPerformanceParameters['Deviation_DeliveredValue'] = dfDeliveryPerformanceParameters['Deviation_DeliveredValue']. \\\n",
    "                                                            round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dfDeliveryPerformanceParameters[dfDeliveryPerformanceParameters['Material'] == 'HT10-100X100X10']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.40]: Build the final dataframe for ML modelling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.41]: Concatenate all the required columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Finalize the columns in dataframe.'\n",
    "    s_statusType        = 'Success'    \n",
    "    # Concat all the columns required for the analysis.\n",
    "    dfDataVendorFinal = dfDataVendorMaxDate.merge(dfDeliveryPerformanceParameters,on=['Material','VendorCode'],how='inner')\n",
    "\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Material', 'VendorCode', 'Deviation_DeliveryDate',\n",
       "       'Deviation_DeliveredQty', 'Deviation_DeliveredValue',\n",
       "       'MaterialCountByVendor'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfDeliveryPerformanceParameters.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.50]: Treat categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Treat Categorical Varaibles'\n",
    "    s_statusType        = 'Success' \n",
    "    # Build the dataframe with the encoded column for the categorical data.\n",
    "    dfMaterialOHE = pd.get_dummies(dfDataVendorFinal['Material'])\n",
    "    dfVendorOHE = pd.get_dummies(dfDataVendorFinal['VendorCode'])\n",
    "    # Add the encoded columns to the existing dataframe.\n",
    "    dfDataVendorFinal = pd.concat([dfDataVendorFinal,dfMaterialOHE,dfVendorOHE],axis=1)\n",
    "    # prepare the rank data by hard coding the weights. \n",
    "    dfDataVendorFinal['Rank'] = ( (dfDataVendorFinal['Deviation_DeliveryDate']*0.4) + \\\n",
    "                                  (dfDataVendorFinal['Deviation_DeliveredQty']*0.4) + \\\n",
    "                                  (dfDataVendorFinal['MaterialCountByVendor']*0.2) \n",
    "                                )\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [1.60]: Machine Learning Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.61]: Split the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Perform Data Split'\n",
    "    s_statusType        = 'Success' \n",
    "\n",
    "    # Copy the dataframe.\n",
    "    dfDataVendorML = dfDataVendorFinal.copy()\n",
    "\n",
    "    # Remove features not useful for the modelling.\n",
    "    featureDrop = ['Doc. Date'\n",
    "                   ,'Material'\n",
    "                   ,'VendorCode'\n",
    "                   ,'UOM'\n",
    "                   ,'Del Date'\n",
    "                   ,'Sum of      Net Price'\n",
    "                   ,'Sum of PO Quantity'\n",
    "                   ,'Sum of PO Value'\n",
    "                   ,'Deviation_DeliveredValue'\n",
    "                   ,'Rank'\n",
    "                   ,'MaterialCountByVendor'\n",
    "                   ]\n",
    "    dfDataVendorML.drop(featureDrop, axis=1,inplace=True)\n",
    "    # Extract the independent variable.\n",
    "    X_Input = dfDataVendorML.copy()\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.62]: Standardize the data for new query point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Standardize the data'\n",
    "    s_statusType        = 'Success' \n",
    "    \n",
    "    # Get the trained standardizer from the pickle.\n",
    "    objType = 'TN'\n",
    "    standardizer    = applConfig.GetTrainedObjectFromPickle(objType)\n",
    "    X_unseen_stdzd  = applConfig.standardize_new_data(dfDataVendorML,\n",
    "                                                      standardizer) \n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.64]: Predict on new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    s_classMethod       = 'Perform Model classification'\n",
    "    s_statusType        = 'Success' \n",
    "\n",
    "    # Get the trained model and calibrated model from the pickle.\n",
    "    objType = 'TM'\n",
    "    lr_optimal    = applConfig.GetTrainedObjectFromPickle(objType)\n",
    "    objType = 'TCM'\n",
    "    calibratedCCV = applConfig.GetTrainedObjectFromPickle(objType)\n",
    "\n",
    "    # Get the predictions from the unseen set.\n",
    "    Y_pred_unseen,Y_pred_proba_unseen = applConfig.GetPredictionsOnUnseenData( lr_optimal,\n",
    "                                                                               X_unseen_stdzd)\n",
    "\n",
    "    # Get the calibrated predictions from the unseen set.\n",
    "    Y_pred_calib_unseen = applConfig.GetCalibratedPredictionsOnUnseenData( calibratedCCV,\n",
    "                                                                           X_unseen_stdzd)\n",
    "\n",
    "except:\n",
    "    s_statusType        = 'Error'\n",
    "    s_statusDescription = str(sys.exc_info())\n",
    "    applConfig.WriteToActivityLog(s_classApplication,\n",
    "                                  s_classMethod,\n",
    "                                  s_statusType,\n",
    "                                  s_statusDescription\n",
    "                                 )    \n",
    "    raise Exception(s_statusDescription)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.65]: Get class probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update class perdictions back to the main dataframe.\n",
    "# Class 0 ==> Has deviations.\n",
    "# Class 1 ==> Has no deviations.\n",
    "dfDataVendorFinal['class_Deviation_score'] = np.round(Y_pred_calib_unseen[:,0],3)\n",
    "# Update class probabilities back to the main dataframe.\n",
    "dfDataVendorFinal['class_NoDeviation_score'] = np.round(Y_pred_calib_unseen[:,1],3)\n",
    "\n",
    "# Compute Is_Deviation flag.\n",
    "dfDataVendorFinal['IsDeviation'] = 'Yes'\n",
    "dfDataVendorFinal.loc[( (dfDataVendorFinal['Deviation_DeliveryDate']   <= 0.0) & \\\n",
    "                        (dfDataVendorFinal['Deviation_DeliveredQty']   == 0.0) \\\n",
    "                      ),  \\\n",
    "                     'IsDeviation'] = 'No'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### [1.67]: Export the result to .csv format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the path to store the result.\n",
    "outputFile = applConfig.Output + 'VendorRanking.csv'\n",
    "# Export it to csv format.\n",
    "dfDataVendorFinal[['Material','VendorCode','UOM','Doc. Date','Del Date' \\\n",
    "                   ,'Sum of PO Quantity','Sum of      Net Price','Sum of PO Value','MaterialCountByVendor','Rank' \\\n",
    "                   ,'Deviation_DeliveryDate', 'Deviation_DeliveredQty' \\\n",
    "                   ,'class_NoDeviation_score','class_Deviation_score' \\\n",
    "                   ,'IsDeviation']].to_csv(outputFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
